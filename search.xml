<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[ansible的一些小tips]]></title>
    <url>%2F2018%2F01%2F12%2Fansible%2F</url>
    <content type="text"><![CDATA[自动化工具当中，一直使用的是ansible。而之所以选择它，不仅仅是因为agentless的便利，也是因为ansible的灵活，ad-hoc的简单易用，role的强大。在使用ansible的过程中，除了常用的各模块的使用方法，还积累了一些用得较少的经验，在此以tips的方式记录在此。 批量建立信任连接1ansible webserver -m authorized_key -a "key=\"&#123;&#123;lookup('file','~/.ssh/id_rsa.pub')&#125;&#125;\" user=$USER" -u user -k 在使用ansible的时候，通常我们都认为它是agentless的，这也是它最吸引人的特点之一。但是严格意义上，ansible也是有agent的。它的agent就是我们熟悉的sshd，只不过sshd是linux的内核标配，所以被大家熟知为无agent的自动化运维工具。因此使用ansible之前，我们虽然不需要对目标主机安装agent，但需要对我们的目标主机做信任连接，这也就等价于安装agent的过程了。对大规模主机做信任连接是件费时费力的事，ansible的authorized_key模块就是为此而生的。代码段中，key的值为公钥的路径，user为目标主机的目标用户，“-k”参数为手动输入密码一次。 动态使用inventory文件一般来说，我们会将服务列表都放在同一个inventory（比如/etc/ansible/hosts）文件中；但在有些特殊应用场景（比如测试的时候），我们往往需要使用临时inventory文件。ansible给我们提供“-i”参数手动指定inventory文件，ansible和ansible-playbook都可以使用这个参数。比如：1ansible -i ./tmp_host all -m shell -a "echo test" ansible forksAnsible与远端节点交流是通过并行的机制，并行机制的方式可以通过传递“–forks=#”或“-f #”参数设置，或者在配置文件里面编辑。默认是保守的5个线程。如果你有足够的内存，你可以很容易的设置为50或者更多值。所以经常有人诟病ansible效率太差，是因为他只用到了默认的5个线程。当需要执行大规模的批量操作时，根据需要调高这个参数即可：1ansible -i ./tmp_host all -m shell -a "echo test" -f 30 快固然好，但有时候，我们也需要让我们的程序慢下来。当我们在做灰度发布的时候，往往是一批一批的更新我们的服务。但当我们的服务集群比较小，比如仅有两个实例的互备服务，我们在发布的时候为了不影响用户的体验，我们可以使用“-f”调小并发数，达到平滑发布的效果。1ansible tomcat_server -m shell -a "sh /usr/local/tomcat/bin/shutdown.sh &amp;&amp; sh /usr/local/tomcat/bin/startup.sh" -f 1 inventory的行为参数我们有时候需要在ansible inventory文件中配置ssh的一些参数，我们需要定义主机名，以及ansible的ssh客户端可以连接到的端口(22,2222,22300)等，那么ansible将这些变量命名为inventory的行为参数，如下：123456789名称 默认值 描述ansible_ssh_host 主机的名字 SSH目的主机名或IPansible_ssh_port 22 SSH目的端口ansible_ssh_user root SSH登录使用的用户名ansible_ssh_pass none SSH认证所使用的密码ansible_connection smart ansible使用何种连接模式连接到主机ansible_ssh_private_key_file none SSH认证所使用的私钥ansible_shell_type sh 命令所使用的shellansible_python_interpreter /usr/bin/python 主机上的python解释器 Group Varsgroup vars写在inventory中：1234567[atlanta]host1host2[atlanta:vars]ntp_server=ntp.atlanta.example.comproxy=proxy.atlanta.example.com group_vars文件也可位于一个目录下面，同时在inventory旁边，有一个可选的文件名在每个组后面。这是一个方便的位置来存放变量，提供给每个组，由其是复杂的数据结构，因此这些变量不需要嵌入在inventory文件或playbook文件里面。123File: /etc/ansible/group_vars/groupadmin_user: tomansible_ssh_pass: password Host Varshost vars写在inventory中：123[atlanta]host1 http_port=80 maxRequestsPerChild=808host2 http_port=303 maxRequestsPerChild=909 就像”Group Vars”，也有一个名称为“host_vars/”的目录在inventory文件旁，可以在invetory文件的主机名后面包含这个文件，使用 YAML 格式。这提供一个方便的位置分配变量给这个主机而不要在inventory文件里面嵌入太多变量。Host Vars 文件还可以用于定义复杂的在inventory文件里面不断出现的数据结构。12File: /etc/ansible/host_vars/host1admin_user: john 异步和轮询ansible有时候要执行等待时间很长的操作,这个操作可能要持续很长时间,设置超过ssh的timeout.这时候你可以在step中指定async和poll来实现异步操作async表示这个step的最长等待时长,如果设置为0,表示一直等待下去直到动作完成.poll 表示检查step操作结果的间隔时长.例一：1234567- name: Test hosts: localhost tasks: - name: wair for shell: sleep 16 async: 10 poll: 2 这个step失败, 因为操作时间超过了最大等待时长例二:1234567- name: Test hosts: localhost tasks: - name: wair for shell: sleep 16 async: 10 poll: 0 结果:1234TASK: [wair for] **************************************************************&lt;job 621720484791.102116&gt; finished on localhostPLAY RECAP ******************************************************************** poll 设置为0, 表示不用等待执行结果, 该step执行成功例三:1234567- name: Test hosts: localhost tasks: - name: wair for shell: sleep 16 async: 0 poll: 10 结果:123456789# time ansible-playbook xiama.ymlTASK: [wair for] **************************************************************changed: [localhost]PLAY RECAP ********************************************************************localhost : ok=2 changed=1 unreachable=0 failed=0real 0m16.693s async设置为0, 会一直等待直到该操作完成. 以上。]]></content>
      <categories>
        <category>技术进步</category>
      </categories>
      <tags>
        <tag>tips</tag>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[next主题fullimage显示不居中问题]]></title>
    <url>%2F2018%2F01%2F01%2Ftest-art%2F</url>
    <content type="text"><![CDATA[使用好hexo和next搭建好自己的博客之后，就到了等待备案的日子. 小小抱怨一下不得不说备案真是一个让人抓狂的过程，之前对公司备案的繁琐有所经历，没想到个人备案也是不遑多让。服务器是在阿里云上，阿里云在备案方面帮着省去了很多的备案步骤，结果还是弄得我够呛，关于备案在之前的文章有详细提及。 遇到的问题这次是在阅读iissnan大神文档的时候，发现了FullImage的功能，觉得很酷炫，就想着用着试试。结果一直没达到文档中的效果，图片的尺寸是超出700px了，但是愣是不居中，效果有点尴尬。 解决思路和过程开始猜测是next尺寸的问题，翻了半天，发现自己并没有使用custom.yml调整过尺寸，并且尴尬的是不论怎么调整尺寸都还是一样。秉持着”有啥不懂看官方文档”的宗旨，把作者的文档来来回回看了两遍，看自己的配置是否有问题，结果无功而返。然后直接搜索引擎搜是否有朋友有过类似的困扰，也没有搜到。然后继续寻找NexT主题相关社区，想着提个问题什么的，也没找到靠谱活跃的社区。最终在GitHub上找到了十几天前有位朋友开了个一模一样的issue #2039，并且已经在#2043得到了解决。 以上。]]></content>
      <categories>
        <category>技术进步</category>
      </categories>
      <tags>
        <tag>NexT</tag>
        <tag>problems</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[time_wait详解[转载]]]></title>
    <url>%2F2017%2F11%2F03%2Fnet_tw%2F</url>
    <content type="text"><![CDATA[time_wait的问题对于经常处理网络问题的同学来说，time_wait是无法回避的问题之一。经常发生的场景是：一旦有用户在反馈“网络变慢了”。第一件做的事情就是：1netstat -a | grep TIME_WAIT | wc -l 一看吓一跳，好几千个TIME_WAIT。然后就是百度或者google，搜索“too many time_wait”，排在最靠前的解决办法，往往就是打开sysctl.conf，修改下面三个参数：123net.ipv4.tcp_tw_recycle = 1net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_timestamps = 1 你也会被告知，开启tw_recylce和tw_reuse一定需要timestamps的支持，而且这些配置一般不建议开启，但是对解决TIME_WAIT很多的问题，有很好的用处。接下来，你就直接修改了这几个参数，reload一下，发现，果然，没几分钟，TIME_WAIT的数量真的降低了，也没发现哪个用户说有问题，然后就没有然后了。做到这一步，相信50%或者更高比例的开发就已经止步了。问题好像解决了，但是，要彻底理解并解决这个问题，可能就没这么简单，或者说，还有很长的路要走！ 什么是TIME-WAIT和CLOSE-WAIT首先，要解决问题，就要先理解问题。随便改两行代码或者配置，发现bug“没有了”，也不是bug真的没有了，只是隐藏在更深的地方，你没有发现，或者以你的知识水平，你无法发现而已。大家知道，由于socket是全双工的工作模式，一个socket的关闭，是需要四次握手来完成的。 * 主动关闭连接的一方，调用close()；协议层发送FIN包 * 被动关闭的一方收到FIN包后，协议层回复ACK；然后被动关闭的一方，进入CLOSE_WAIT状态，主动关闭的一方等待对方关闭，则进入FIN_WAIT_2状态；此时，主动关闭的一方 等待 被动关闭一方的应用程序，调用close操作 * 被动关闭的一方在完成所有数据发送后，调用close()操作；此时，协议层发送FIN包给主动关闭的一方，等待对方的ACK，被动关闭的一方进入LAST_ACK状态； * 主动关闭的一方收到FIN包，协议层回复ACK；此时，主动关闭连接的一方，进入TIME_WAIT状态；而被动关闭的一方，进入CLOSED状态 * 等待2MSL时间，主动关闭的一方，结束TIME_WAIT，进入CLOSED状态 通过上面的一次socket关闭操作，你可以得出以下几点： 1. 主动关闭连接的一方 - 也就是主动调用socket的close操作的一方，最终会进入TIME_WAIT状态 2. 被动关闭连接的一方，有一个中间状态，即CLOSE_WAIT，因为协议层在等待上层的应用程序，主动调用close操作后才主动关闭这条连接 3. TIME_WAIT会默认等待2MSL时间后，才最终进入CLOSED状态； 4. 在一个连接没有进入CLOSED状态之前，这个连接是不能被重用的！ 所以，这里凭你的直觉，TIME_WAIT并不可怕（not really，后面讲），CLOSE_WAIT才可怕，因为CLOSE_WAIT很多，表示说要么是你的应用程序写的有问题，没有合适的关闭socket；要么是说，你的服务器CPU处理不过来（CPU太忙）或者你的应用程序一直睡眠到其它地方(锁，或者文件I/O等等)，你的应用程序获得不到合适的调度时间，造成你的程序没法真正的执行close操作。这里又出现两个问题： 1. 上文提到的连接重用，那连接到底是个什么概念？ 2. 协议层为什么要设计一个TIME_WAIT状态？这个状态为什么默认等待2MSL时间才会进入CLOSED 先解释清楚这两个问题，我们再来看，开头提到的几个网络配置究竟有什么用，以及TIME_WAIT的后遗症问题。 Socket连接大家经常提socket，那么，到底什么是一个socket？其实，socket就是一个 五元组，包括： 1. 源IP 2. 源端口 3. 目的IP 4. 目的端口 5. 类型：TCP or UDP 这个五元组，即标识了一条可用的连接。注意，有很多人把一个socket定义成四元组，也就是 源IP:源端口 + 目的IP:目的端口，这个定义是不正确的。 例如，如果你的本地出口IP是180.172.35.150，那么你的浏览器在连接某一个Web服务器，例如百度的时候，这条socket连接的四元组可能就是： [180.172.35.150:45678, tcp, 180.97.33.108:80] 源IP为你的出口IP地址 180.172.35.150，源端口为随机端口 45678，目的IP为百度的某一个负载均衡服务器IP 180.97.33.108，端口为HTTP标准的80端口。 如果这个时候，你再开一个浏览器，访问百度，将会产生一条新的连接： [180.172.35.150:43678, tcp, 180.97.33.108:80] 这条新的连接的源端口为一个新的随机端口 43678。 TIME_WAIT的意义如果我们来做个类比的话，TIME_WAIT的出现，对应的是你的程序里的异常处理，它的出现，就是为了解决网络的丢包和网络不稳定所带来的其他问题： 第一，防止前一个连接【五元组，我们继续以 180.172.35.150:45678, tcp, 180.97.33.108:80 为例】上延迟的数据包或者丢失重传的数据包，被后面复用的连接【前一个连接关闭后，此时你再次访问百度，新的连接可能还是由180.172.35.150:45678, tcp, 180.97.33.108:80 这个五元组来表示，也就是源端口凑巧还是45678】错误的接收（异常：数据丢了，或者传输太慢了），参见下图： * SEQ=3的数据包丢失，重传第一次，没有得到ACK确认 * 如果没有TIME_WAIT，或者TIME_WAIT时间非常端，那么关闭的连接【180.172.35.150:45678, tcp, 180.97.33.108:80 的状态变为了CLOSED，源端口可被再次利用】，马上被重用【对180.97.33.108:80新建的连接，复用了之前的随机端口45678】，并连续发送SEQ=1,2 的数据包 * 此时，前面的连接上的SEQ=3的数据包再次重传，同时，seq的序号刚好也是3（这个很重要，不然，SEQ的序号对不上，就会RST掉），此时，前面一个连接上的数据被后面的一个连接错误的接收 第二，确保连接方能在时间范围内，关闭自己的连接。其实，也是因为丢包造成的，参见下图： * 主动关闭方关闭了连接，发送了FIN； * 被动关闭方回复ACK同时也执行关闭动作，发送FIN包；此时，被动关闭的一方进入LAST_ACK状态 * 主动关闭的一方回去了ACK，主动关闭一方进入TIME_WAIT状态； * 但是最后的ACK丢失，被动关闭的一方还继续停留在LAST_ACK状态 * 此时，如果没有TIME_WAIT的存在，或者说，停留在TIME_WAIT上的时间很短，则主动关闭的一方很快就进入了CLOSED状态，也即是说，如果此时新建一个连接，源随机端口如果被复用，在connect发送SYN包后，由于被动方仍认为这条连接【五元组】还在等待ACK，但是却收到了SYN，则被动方会回复RST * 造成主动创建连接的一方，由于收到了RST，则连接无法成功 所以，你看到了，TIME_WAIT的存在是很重要的，如果强制忽略TIME_WAIT，还是有很高的机率，造成数据粗乱，或者短暂性的连接失败。 那么，为什么说，TIME_WAIT状态会是持续2MSL（2倍的max segment lifetime）呢？这个时间可以通过修改内核参数调整吗？第一，这个2MSL，是RFC 793里定义的，参见RFC的截图标红的部分： TIME_WAIT对资源的占用如果你通过 ss -tan state time-wait | wc -l 发现，系统中有很多TIME_WAIT，很多人都会紧张。多少算多呢？几百几千？如果是这个量级，其实真的没必要紧张。第一，这个量级，因为TIME_WAIT所占用的内存很少很少；因为记录和寻找可用的local port所消耗的CPU也基本可以忽略。 会占用内存吗？当然！任何你可以看到的数据，内核里都需要有相关的数据结构来保存这个数据啊。一条Socket处于TIME_WAIT状态，它也是一条“存在”的socket，内核里也需要有保持它的数据： 1. 内核里有保存所有连接的一个hash table，这个hash table里面既包含TIME_WAIT状态的连接，也包含其他状态的连接。主要用于有新的数据到来的时候，从这个hash table里快速找到这条连接。不同的内核对这个hash table的大小设置不同，你可以通过dmesg命令去找到你的内核设置的大小： 2. 还有一个hash table用来保存所有的bound ports，主要用于可以快速的找到一个可用的端口或者随机端口： 由于内核需要保存这些数据，必然，会占用一定的内存。会消耗CPU吗？当然！每次找到一个随机端口，还是需要遍历一遍bound ports的吧，这必然需要一些CPU时间。TIME_WAIT很多，既占内存又消耗CPU，这也是为什么很多人，看到TIME_WAIT很多，就蠢蠢欲动的想去干掉他们。其实，如果你再进一步去研究，1万条TIME_WAIT的连接，也就多消耗1M左右的内存，对现代的很多服务器，已经不算什么了。至于CPU，能减少它当然更好，但是不至于因为1万多个hash item就担忧。如果，你真的想去调优，还是需要搞清楚别人的调优建议，以及调优参数背后的意义！ TIME_WAIT调优的几个参数在具体的图例之前，我们还是先解析一下相关的几个参数存在的意义。 net.ipv4.tcp_timestampsRFC 1323 在 TCP Reliability一节里，引入了timestamp的TCP option，两个4字节的时间戳字段，其中第一个4字节字段用来保存发送该数据包的时间，第二个4字节字段用来保存最近一次接收对方发送到数据的时间。有了这两个时间字段，也就有了后续优化的余地。 tcp_tw_reuse 和 tcp_tw_recycle就依赖这些时间字段。 net.ipv4.tcp_tw_reuse字面意思，reuse TIME_WAIT状态的连接。 时刻记住一条socket连接，就是那个五元组，出现TIME_WAIT状态的连接，一定出现在主动关闭连接的一方。所以，当主动关闭连接的一方，再次向对方发起连接请求的时候（例如，客户端关闭连接，客户端再次连接服务端，此时可以复用了；负载均衡服务器，主动关闭后端的连接，当有新的HTTP请求，负载均衡服务器再次连接后端服务器，此时也可以复用），可以复用TIME_WAIT状态的连接。 通过字面解释，以及例子说明，你看到了，tcp_tw_reuse应用的场景：某一方，需要不断的通过“短连接”连接其他服务器，总是自己先关闭连接(TIME_WAIT在自己这方)，关闭后又不断的重新连接对方。 那么，当连接被复用了之后，延迟或者重发的数据包到达，新的连接怎么判断，到达的数据是属于复用后的连接，还是复用前的连接呢？那就需要依赖前面提到的两个时间字段了。复用连接后，这条连接的时间被更新为当前的时间，当延迟的数据达到，延迟数据的时间是小于新连接的时间，所以，内核可以通过时间判断出，延迟的数据可以安全的丢弃掉了。 这个配置，依赖于连接双方，同时对timestamps的支持。同时，这个配置，仅仅影响outbound连接，即做为客户端的角色，连接服务端[connect(dest_ip, dest_port)]时复用TIME_WAIT的socket。 net.ipv4.tcp_tw_recycle字面意思，销毁掉 TIME_WAIT。 当开启了这个配置后，内核会快速的回收处于TIME_WAIT状态的socket连接。多快？不再是2MSL，而是一个RTO（retransmission timeout，数据包重传的timeout时间）的时间，这个时间根据RTT动态计算出来，但是远小于2MSL。 有了这个配置，还是需要保障 丢失重传或者延迟的数据包，不会被新的连接(注意，这里不再是复用了，而是之前处于TIME_WAIT状态的连接已经被destroy掉了，新的连接，刚好是和某一个被destroy掉的连接使用了相同的五元组而已)所错误的接收。在启用该配置，当一个socket连接进入TIME_WAIT状态后，内核里会记录包括该socket连接对应的五元组中的对方IP等在内的一些统计数据，当然也包括从该对方IP所接收到的最近的一次数据包时间。当有新的数据包到达，只要时间晚于内核记录的这个时间，数据包都会被统统的丢掉。 这个配置，依赖于连接双方对timestamps的支持。同时，这个配置，主要影响到了inbound的连接（对outbound的连接也有影响，但是不是复用），即做为服务端角色，客户端连进来，服务端主动关闭了连接，TIME_WAIT状态的socket处于服务端，服务端快速的回收该状态的连接。 由此，如果客户端处于NAT的网络(多个客户端，同一个IP出口的网络环境)，如果配置了tw_recycle，就可能在一个RTO的时间内，只能有一个客户端和自己连接成功(不同的客户端发包的时间不一致，造成服务端直接把数据包丢弃掉)。 实践案例和调优参数我尽量尝试用文字解释清楚，但是，来点案例和图示，应该有助于我们彻底理解。 我们来看这样一个网络情况： 1. 客户端IP地址为：180.172.35.150，我们可以认为是浏览器 2. 负载均衡有两个IP，外网IP地址为 115.29.253.156，内网地址为10.162.74.10；外网地址监听80端口 3. 负载均衡背后有两台Web服务器，一台IP地址为 10.162.74.43，监听80端口；另一台为 10.162.74.44，监听 80 端口 4. Web服务器会连接数据服务器，IP地址为 10.162.74.45，监听 3306 端口 这种简单的架构下，我们来看看，在不同的情况下，我们今天谈论的tw_reuse/tw_recycle对网络连接的影响。 先做个假定： 1. 客户端通过HTTP/1.1连接负载均衡，也就是说，HTTP协议投Connection为keep-alive，所以我们假定，客户端 对 负载均衡服务器 的socket连接，客户端会断开连接，所以，TIME_WAIT出现在客户端 2. Web服务器和MySQL服务器的连接，我们假定，Web服务器上的程序在连接结束的时候，调用close操作关闭socket资源连接，所以，TIME_WAIT出现在 Web 服务器端。 那么，在这种假定下： 1. Web服务器上，肯定可以配置开启的配置：tcp_tw_reuse；如果Web服务器有很多连向DB服务器的连接，可以保证socket连接的复用。 2. 那么，负载均衡服务器和Web服务器，谁先关闭连接，则决定了我们怎么配置tcp_tw_reuse/tcp_tw_recycle了 方案一：负载均衡服务器 首先关闭连接 在这种情况下，因为负载均衡服务器对Web服务器的连接，TIME_WAIT大都出现在负载均衡服务器上，所以，在负载均衡服务器上的配置： * net.ipv4.tcp_tw_reuse = 1 //尽量复用连接 * net.ipv4.tcp_tw_recycle = 0 //不能保证客户端不在NAT的网络啊 在Web服务器上的配置为： * net.ipv4.tcp_tw_reuse = 1 //这个配置主要影响的是Web服务器到DB服务器的连接复用 * net.ipv4.tcp_tw_recycle： 设置成1和0都没有任何意义。想一想，在负载均衡和它的连接中，它是服务端，但是TIME_WAIT出现在负载均衡服务器上；它和DB的连接，它是客户端，recycle对它并没有什么影响，关键是reuse 方案二：Web服务器首先关闭来自负载均衡服务器的连接 在这种情况下，Web服务器变成TIME_WAIT的重灾区。负载均衡对Web服务器的连接，由Web服务器首先关闭连接，TIME_WAIT出现在Web服务器上；Web服务器对DB服务器的连接，由Web服务器关闭连接，TIME_WAIT也出现在它身上，此时，负载均衡服务器上的配置： * net.ipv4.tcp_tw_reuse：0 或者 1 都行，都没有实际意义 * net.ipv4.tcp_tw_recycle=0 //一定是关闭recycle 在Web服务器上的配置： * net.ipv4.tcp_tw_reuse = 1 //这个配置主要影响的是Web服务器到DB服务器的连接复用 * net.ipv4.tcp_tw_recycle=1 //由于在负载均衡和Web服务器之间并没有NAT的网络，可以考虑开启recycle，加速由于负载均衡和Web服务器之间的连接造成的大量TIME_WAIT 以上。]]></content>
      <categories>
        <category>技术进步</category>
      </categories>
      <tags>
        <tag>time_wait</tag>
        <tag>网络</tag>
        <tag>排错</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins+ansible持续集成之自动发布脚本]]></title>
    <url>%2F2017%2F08%2F03%2Fansi-ci%2F</url>
    <content type="text"><![CDATA[工作中使用jenkins已经很久了，很早就想总结一下自己的一些使用情况，本文主要记录一下自动发布的模板脚本。 首先说明jenkins无疑是近几年来最炙手可热的持续集成工具，功能强大，插件应有尽有，社区活跃，且被众多大公司实际生产中应用。关于jenkins，在此就不详细介绍了（主要是我也还远没了解透彻）。要想详细了解jenkins，请直接查看官方文档有详细提及。 主要应用点jenkins是一个功能强大的工具，但我司目前还只是用来做自动拉取代码、打包、发布这几项工作，主要涉及的工具是jenkins和ansible。使用jenkins自动拉取代码，自动构建，ansible完成发布工作。 简单讲解jenkins中主要是使用了maven（针对maven项目）和git两个插件.ansible则可以使用command_line、play_book或者role都可以。由于应用环境简单，我直接使用了ansible的command_line。 jenkins中的shell1sh /deploy/jenkins_deploy_$&#123;JOB_NAME&#125;.sh jenkins-deploy-${JOB_NAME}.sh123456789101112131415161718192021#定义应用变量和应用目录，发布其它应用只需要把APP的值修改成相应的,比如APP=demo.war#需要修改APP这一处即可APP=app_name#定义发布应用的脚本变量，发布其它应用需要把DEPLOY_APP的值修改成相应的，DEPLOY_APP=deploy_$&#123;APP&#125;.sh#定义部署服务器组webservers=$&#123;APP&#125;DEP_PATH=dep_pathPAK_PATH=pak_path#应用服务器创建deploy目录ansible $webservers -u appsvr -m shell -a "mkdir -p $&#123;DEP_PATH&#125;"#将war包复制到应用服务器的/deployansible $webservers -u appsvr -m copy -a "src=$&#123;PAK_PATH&#125;/$&#123;APP&#125;-1.0-SNAPSHOT.jar dest=$&#123;DEP_PATH&#125; owner=appsvr group=appsvr mode=0755 "#将构建ID号复制到应用服务器的/deploy下#ansible $webservers -u qappsom -m copy -a "src=/deploy/$LASTBUILD dest=/deploy/ owner=qappsom group=grpadm mode=0755 "#将部署脚本复制到应用服务器的/deploy下ansible $webservers -u appsvr -m copy -a "src=$&#123;DEP_PATH&#125;/$DEPLOY_APP dest=$&#123;DEP_PATH&#125; owner=appsvr group=appsvr mode=0644 "#执行发布脚本ansible $webservers -u appsvr -m shell -a "sh /$&#123;DEP_PATH&#125;/$&#123;DEPLOY_APP&#125;" -f 1 deploy_${JOB_NAME}.sh1234567891011121314151617181920212223242526272829303132333435363738#如果新增模块，只需复制本脚本，然后把APP的变量值改成相对应的模块名即可。比如,APP=oms#只需要改1处source /etc/profileAPP=app_nameNOW=`date +%Y%m%d-%H%M%S`BACKUP_PATH=back_pathAPP_PATH=app_pathDEP_PATH=dep_pathJAR_NAME=$&#123;APP&#125;-xxx.jar###先确定是否存在备份目录，不存在则创建if [ ! -d "$&#123;BACKUP_PATH&#125;" ]then mkdir -p $&#123;BACKUP_PATH&#125;fi###备份原有jar包，加上时间后缀if [ -f "$&#123;APP_PATH&#125;/$&#123;JAR_NAME&#125;" ]then cp -v $&#123;APP_PATH&#125;/$&#123;JAR_NAME&#125; $&#123;BACKUP_PATH&#125;/$&#123;JAR_NAME&#125;_bak$&#123;NOW&#125; echo "backup is done !"else mkdir -p $&#123;APP_PATH&#125; echo "this is the first time delpoying the $APP !"fi###覆盖应用jar包，重启服务cp -v $&#123;DEP_PATH&#125;$&#123;JAR_NAME&#125; $&#123;APP_PATH&#125;/echo "copying $&#123;JAR_NAME&#125; to $&#123;APP_PATH&#125; down!"until ! [ $(ps -ef|grep "$&#123;JAR_NAME&#125;"|grep -v grep|awk '&#123;print$2&#125;') ];do echo -e "\033[32m [INFO]: killing the $&#123;APP&#125; process ... \033[0m" ps -ef|grep "$&#123;JAR_NAME&#125;"|grep -v grep|awk '&#123;print$2&#125;'|xargs kill sleep 3doneecho "restarting new $&#123;APP&#125; process..."nohup $&#123;JAVA_HOME&#125;/bin/java -jar $&#123;APP_PATH&#125;/$&#123;JAR_NAME&#125; &gt; /dev/null 2&gt;&amp;1 &amp;echo "The $&#123;APP&#125; has stated ! "ps -ef|grep "$&#123;JAR_NAME&#125;"|grep -v grepexit 以上。]]></content>
      <categories>
        <category>技术进步</category>
      </categories>
      <tags>
        <tag>ansible</tag>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GlusterFS常用命令]]></title>
    <url>%2F2017%2F06%2F05%2Fgfs_cmd%2F</url>
    <content type="text"><![CDATA[生产中不可避免地用到了NFS，但对于一些重要的数据需要共享的时候，NFS显得不是那么保险。所以在众多开源项目中寻找了分布式存储系统GlusterFS作为NFS的替代。GlusterFS也被直接用到了自行设计的yummy-config配置文件关系系统中，不仅更好地保证了文件的安全，也在性能上有了很好的改善。本文主要记录一些常用的GlsuterFS的command_line。 删除卷12# gluster volume stop gv1# gluster volume delete gv1 将一台server移出整个集群1# gluster peer detach server4 卷扩容12345678910111213141516171819202122#首先先添加servergluster peer probe server5 #添加server5gluster peer probe server6 #添加server6#接下来向volume中add brickgluster volume add-brick gv1 server5:/bricks/brick1/gv1 server6:/brick/brick1/gv1收缩卷（在卷收缩之前gluster需要先移动数据到其他位置）：#首先remove brickgluster volume remove-brick gv1 server5:/bricks/brick1/gv1 server6:/brick/brick1/gv1 start#查看迁移状态gluster volume remove-brick gv1 server5:/bricks/brick1/gv1 server6:/brick/brick1/gv1 status#迁移完成后提交gluster volume remove-brick gv1 server5:/bricks/brick1/gv1 server6:/brick/brick1/gv1 commit 迁移卷（将server4上的数据迁移到server5上）12345678910111213141516171819#首先将server5添加到TSP中gluster peer probe server5#接着开始迁移gluster volume replace-brick gv1 server4:/bricks/brick1/gv1 server5:/brick/brick1/gv1 start#查看迁移状态gluster volume replace-brick gv1 server4:/bricks/brick1/gv1 server5:/brick/brick1/gv1 status#数据迁移完成之后提交gluster volume replace-brick gv1 server4:/bricks/brick1/gv1 server5:/brick/brick1/gv1 commit#如果server4出现故障已经不能运行，执行强制提交gluster volume replace-brick gv1 server4:/bricks/brick1/gv1 server5:/brick/brick1/gv1 commit -force 同步整个卷1# gluster volume heal gv1 full 触发副本自愈1234567#只修复有问题的文件gluster volume heal mamm-volume#修复所有文件gluster volume heal mamm-volume full#查看自愈详情gluster volume heal mamm-volume infogluster volume heal mamm-volume info healed|heal-failed|split-brain]]></content>
      <categories>
        <category>技术进步</category>
      </categories>
      <tags>
        <tag>GlusterFS</tag>
        <tag>分布式存储</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GlusterFS简介和可用性测试]]></title>
    <url>%2F2017%2F06%2F03%2Fglusterfs%2F</url>
    <content type="text"><![CDATA[Glusterfs简介GlusterFS是Scale-Out存储解决方案Gluster的核心，它是一个开源的分布式文件系统，具有强大的横向扩展能力，通过扩展能够支持数PB存储容量和处理数千客户端。GlusterFS借助TCP/IP或InfiniBand RDMA（ 远程直接数据存取）网络将物理分布的存储资源聚集在一起，使用单一全局命名空间来管理数据。GlusterFS基于可堆叠的用户空间设计，可为各种不同的数据负载提供优异的性能。 Glusterfs特点扩展性和高性能GlusterFS利用双重特性来提供几TB至数PB的高扩展存储解决方案。Scale-Out架构允许通过简单地增加资源来提高存储容量和性能，磁盘、计算和I/O资源都可以独立增加，支持10GbE和InfiniBand等高速网络互联。Gluster弹性哈希（Elastic Hash）解除了GlusterFS对元数据服务器的需求，消除了单点故障和性能瓶颈，真正实现了并行化数据访问。 高可用性GlusterFS可以对文件进行自动复制，如镜像或多次复制，从而确保数据总是可以访问，甚至是在硬件故障的情况下也能正常访问。自我修复功能能够把数据恢复到正确的状态，而且修复是以增量的方式在后台执行，几乎不会产生性能负载。GlusterFS没有设计自己的私有数据文件格式，而是采用操作系统中主流标准的磁盘文件系统（如EXT，ZFS）来存储文件，因此数据可以使用各种标准工具进行复制和访问。 全局统一命名空间全局统一命名空间将磁盘和内存资源聚集成一个单一的虚拟存储池，对上层用户和应用屏蔽了底层的物理硬件。存储资源可以根据需要在虚拟存储池中进行弹性扩展，比如扩容或收缩。当存储虚拟机映像时，存储的虚拟映像文件没有数量限制，成千虚拟机均通过单一挂载点进行数据共享。虚拟机I/O可在命名空间内的所有服务器上自动进行负载均衡，消除了SAN环境中经常发生的访问热点和性能瓶颈问题。 弹性哈希算法GlusterFS采用弹性哈希算法在存储池中定位数据，而不是采用集中式或分布式元数据服务器索引。在其他的Scale-Out存储系统中，元数据服务器通常会导致I/O性能瓶颈和单点故障问题。GlusterFS中，所有在Scale-Out存储配置中的存储系统都可以智能地定位任意数据分片，不需要查看索引或者向其他服务器查询。这种设计机制完全并行化了数据访问，实现了真正的线性性能扩展。 弹性卷管理数据储存在逻辑卷中，逻辑卷可以从虚拟化的物理存储池进行独立逻辑划分而得到。存储服务器可以在线进行增加和移除，不会导致应用中断。逻辑卷可以在所有配置服务器中增长和缩减，可以在不同服务器迁移进行容量均衡，或者增加和移除系统，这些操作都可在线进行。文件系统配置更改也可以实时在线进行并应用，从而可以适应工作负载条件变化或在线性能调优。 基于标准协议Gluster存储服务支持NFS, CIFS, HTTP, FTP以及Gluster原生协议，完全与POSIX标准兼容。现有应用程序不需要作任何修改或使用专用API，就可以对Gluster中的数据进行访问。这在公有云环境中部署Gluster时非常有用，Gluster对云服务提供商专用API进行抽象，然后提供标准POSIX接口。 glusterfs整体工作流程整体流程如下图所示： 首先是在客户端， 用户通过glusterfs的mount point 来读写数据， 对于用户来说，集群系统的存在对用户是完全透明的，用户感觉不到是操作本地系统还是远端的集群系统。 用户的这个操作被递交给 本地linux系统的VFS来处理。 VFS 将数据递交给FUSE 内核文件系统:在启动 glusterfs 客户端以前，需要想系统注册一个实际的文件系统FUSE,如上图所示，该文件系统与ext3在同一个层次上面， ext3 是对实际的磁盘进行处理， 而fuse 文件系统则是将数据通过/dev/fuse 这个设备文件递交给了glusterfs client端。所以， 我们可以将 fuse文件系统理解为一个代理。 数据被fuse 递交给Glusterfs client 后， client 对数据进行一些指定的处理（所谓的指定，是按照client 配置文件据来进行的一系列处理， 我们在启动glusterfs client 时需要指定这个文件。 在glusterfs client的处理末端，通过网络将数据递交给 Glusterfs Server，并且将数据写入到服务器所控制的存储设备上。 几种volume（卷）的方式distributed volume（分布巻） 分布卷可以将某个文件随机的存储在卷内的一个brick内，通常用于扩展存储能力，不支持数据的冗余。除非底层的brick使用RAID等外部的冗余措施。命令：1$ gluster volume create mamm-volume node1:/media node2:/media node3:/media ... replicated volume（镜像卷） 镜像卷在创建时可指定复本的数量，复本在存储时会在卷的不同brick上，因此有几个复本就必须提供至少多个brick。1$ gluster volume create mamm-volume repl 2 node1:/media node2:/media 注意：在创建复本卷时，brick数量与复本个数必须相等；否则将会报错。另外如果同一个节点提供了多个brick，也可以在同一个结点上创建复本卷，但这并不安全，因为一台设备挂掉，其上面的所有brick就无法访问了。 striped volume（切片卷） 分片卷将单个文件分成小块(块大小支持配置,默认为128K)，然后将小块存储在不同的brick上，以提升文件的访问性能。1$ gluster volume create mamm-volume stripe 2 node1:/media node2:/media stripe后的参数指明切片的分布位置个数注意：brick的个数必须等于分布位置的个数 distribute replication volume（分布式镜像卷） 此类型卷是基本复本卷的扩展，可以指定若干brick组成一个复本卷，另外若干brick组成另个复本卷。单个文件在复本卷内数据保持复制，不同文件在不同复本卷之间进行分布。1$ gluster volume create dr-volume repl 2 node1:/exp1 node2:/exp2 node3:/exp3 node4:/exp4 注意：复本卷的组成依赖于指定brick的顺序brick必须为复本数K的N倍,brick列表将以K个为一组，形成N个复本卷 distribute striped volume 类似于分布式复本卷，若创建的卷的节点提供的bricks个数为stripe个数N倍时，将创建此类型的卷。1$ gluster volume create ds-volume stripe 2 node1:/exp1 node1:/exp2 [&amp;] node2:/exp3 node2:/exp4 注意：切片卷的组成依赖于指定brick的顺序brick必须为复本数K的N倍,brick列表将以K个为一组，形成N个切片卷 striped replicated volume（切片镜像卷） 数据将进行切片，切片在复本卷内进行复制，在不同卷间进行分布。1$ gluster volume create test-volume stripe 2 replica 2 server1:/exp1 server2:/exp2 server3:/exp3 server4:/exp4 exp1和exp2组成复本卷，exp3和exp4组成复本卷，两个复本卷组成分片卷。注意：brick数量必须和stripe个数N和repl参数M的积N*M相等。即对于brick列表，将以M为一组，形成N个切片卷。数据切片分布在N个切片卷上，在每个切片卷内部，切片数据复本M份。 distributed striped replicated vloume（分布式切片镜像卷） 1$ gluster volume create test-volume stripe 2 replica 2 server1:/exp1 server2:/exp2 server3:/exp3 server4:/exp4 server5:/exp5 server6:/exp6 server7:/exp7 server8:/exp8 注意：bricks数量为stripe个数N，和repl个数M的积N*M的整数倍exp1 exp2 exp3 exp4组成一个分布卷，exp1和exp2组成一个stripe卷，exp3和exp4组成另一个stripe卷，1和2，3和4互为复本卷exp4-exp8组成另一个分布卷，略。 读写测试数据及与NFS的比较测试背景： 两者都是使用单客户端请求读写； GlusterFS采用4台2G/2核，20G高性能硬盘的主机，按照分布式切片卷的的方式挂载使用； NFS采用一台2G/2核，20G高性能硬盘的主机，正常RW挂载使用； 从初步测试数据来看，GlusterFS相对于NFS，在读写性能（尤其是写性能）上，有了不错的提升。后期会尝试采用多客户请求，以及其他更多测试工具来进一步测试。 结论：初步看来，GlusterFS在NFS的基础上，在I/O性能，数据可靠性上都有了很好的提升。]]></content>
      <categories>
        <category>技术进步</category>
      </categories>
      <tags>
        <tag>GlusterFS</tag>
        <tag>分布式存储</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[建站请绕道：备案辛酸路]]></title>
    <url>%2F2017%2F03%2F13%2Fbeian%2F</url>
    <content type="text"><![CDATA[自建blog的想法是早就有了的，但是无奈deadline才是第一生产力，这个事情一直拖到前一段时间才得以真正开展。本以为当hexo和NexT两个工具选好了之后，建站对我这个运维来说，不就是分分钟的事情么？可是…我终究还是naive啊。 响应党的号召：备案光荣在namecheap上成功买了域名（PS.不得不说namecheap的服务是真的比狗爹好多了）；就着手在aliyun上买ecs，结果一犯二就买了华南的服务器，当时我并不知道我将会面对什么。修改好dns_server，待其生效便调试了起来，然后就收到党的号召，需要备案。一直以来，我都是一个根正苗红，又红又专的好青年，对于党的这种无理的要求，你问我支不支持，我当然支持。于是我就走上了备案这条不归路。 阿里云建站许久之前刚入行的时候，记得那时候备案是真心麻烦，需要带着你的各种证件，去到专门的“有关部门”，各种拍照，审查，必须经历很长的周期才能成功。现在因为是在阿里云买了服务器，阿里也和审查机构简化了一些流程，现在只需要在阿里云上upload一些文件和证件，似乎就可以备案成功了。 一波三折之所以是“似乎”，是当我开开心心上传各种资料的时候，发现除了身份证、手机号之外，赫然还需要居住证。当时我就蒙蔽了，“身份证信息如非上海市需提供上海市‘临时居住证’或‘居住证’”。难道还专门为了这事去有关部门办一个暂住证？知乎上一问，发现并不是所有地区的备案都需要暂住证，四川、福建等地方是不需要的。所以备案时你所在的地址填这些地方即可。然后顺利拍摄好各种资料上传完成，可不到一天，就被短信告知备案初审就没通过。还是要夸一下阿里云的工作人员的服务，第一时间电话和我沟通，非常仔细地告诉我哪些地方需要修改，然后还给了我一些小建议。还能怎么办呢？挨个解决问题呗。 域名持有者与主办单位不一致其实就是我在namecheap上填写的所有者的名字并不是我的真名（全拼也可以），关于域名的所有者，到任意一个whois网站上可以查询到域名的所有者，当然要放开namecheap上的whois privacy才能查到，你可以试试自己的域名的whois是谁。在namecheap的控制页上，找了半天还找不到修改域名所有者的操作，索性直接找了他们的客服帮忙修改，服务态度也是很好，虽然我的英语是真的烂。 网站名称不合格网站名称不能为“xxx的个人网站”等各种要求，随便改一个名字就可以了。 未取得备案号禁止访问我在申请好域名，配置好DNS之后就尝试着起了web服务调试了起来，好吧，这也是不行的，先关闭呗。 再次提交改好了以上三项，再次提交，很顺利地通过了初审。然后过了大概三天，通知我已经提交管局审核，看起来一切都是那么顺利。结果一天之后，再次传来审核失败的消息。“域名不存在注册商验证库中”，简单来说呢，就是域名是国外的，是有关部门无法掌控的，有关部门希望你把域名转回到国内的服务商来托管。简直了，当时心里有句MMP，也就大声讲了一万多遍。所以摆在我面前的唯一出路，就是将放在namecheap上的域名转回到万网上来。出路是有出路，可是怎么就感觉那么操蛋呢？我只是想简简单单弄个小网站而已…最后我毅然选择不走这条转域名的唯一出路，又弄了一台香港的服务器，项目往上一部署，DNS一修改，齐活。备案？爱谁谁吧！ 经验总结写这么多，经验总结就是老老实实域名服务器都买国外的吧，别把时间浪费在备案上面了。 以上。]]></content>
      <categories>
        <category>杂谈</category>
      </categories>
      <tags>
        <tag>blog</tag>
        <tag>建站</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2016%2F12%2F11%2Fhello-world%2F</url>
    <content type="text"><![CDATA[这是个人网站的第一篇博客，标题就直接用了‘Hello World’。刚开始接触CS的时候，总是略微有些嫌弃‘Hello World’的，可能是这个梗被用得太多，有点嚼烂了的感觉。不过随着时间的推移，越来越喜欢‘Hello world’这个词了。因为每次屏幕上出现的‘Hello World’，总是伴随着新知识的学习，或是新作品的诞生！就像棋手落下的第一子、画家挥下的第一笔…矫情的废话不多说，James’s note主要会记录以下的一些内容。 Tech BlogJames’s note最最主要的作用就是记录关于技术方面的博客，文章，思考，也就是类似于笔记本的作用。关于技术的，不论是长篇大论，还是自己工作中接触到问题的反思和总结,抑或是一个代码片段，都会记录在这边。 Note Life另一方面，主要会记录自己的一些生活经历，出行，有感等等。 Something Else最后，就是一些七七八八的，想要随便写写的杂文了。 At Last希望自己能够多坚持一些，长期，持续地维护好这个主页，在这期间也能持续地不断成长！ More info: HomePage 以上。]]></content>
      <categories>
        <category>杂谈</category>
      </categories>
      <tags>
        <tag>blog</tag>
        <tag>NexT</tag>
      </tags>
  </entry>
</search>
