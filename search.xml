<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[ansible的一些小tips]]></title>
    <url>%2F2018%2F01%2F12%2Fansible%2F</url>
    <content type="text"><![CDATA[自动化工具当中，一直使用的是ansible。而之所以选择它，不仅仅是因为agentless的便利，也是因为ansible的灵活，ad-hoc的简单易用，role的强大。在使用ansible的过程中，除了常用的各模块的使用方法，还积累了一些用得较少的经验，在此以tips的方式记录在此。 批量建立信任连接1ansible webserver -m authorized_key -a "key=\"&#123;&#123;lookup('file','~/.ssh/id_rsa.pub')&#125;&#125;\" user=$USER" -u user -k 在使用ansible的时候，通常我们都认为它是agentless的，这也是它最吸引人的特点之一。但是严格意义上，ansible也是有agent的。它的agent就是我们熟悉的sshd，只不过sshd是linux的内核标配，所以被大家熟知为无agent的自动化运维工具。因此使用ansible之前，我们虽然不需要对目标主机安装agent，但需要对我们的目标主机做信任连接，这也就等价于安装agent的过程了。对大规模主机做信任连接是件费时费力的事，ansible的authorized_key模块就是为此而生的。代码段中，key的值为公钥的路径，user为目标主机的目标用户，“-k”参数为手动输入密码一次。 动态使用inventory文件一般来说，我们会将服务列表都放在同一个inventory（比如/etc/ansible/hosts）文件中；但在有些特殊应用场景（比如测试的时候），我们往往需要使用临时inventory文件。ansible给我们提供“-i”参数手动指定inventory文件，ansible和ansible-playbook都可以使用这个参数。比如：1ansible -i ./tmp_host all -m shell -a "echo test" ansible forksAnsible与远端节点交流是通过并行的机制，并行机制的方式可以通过传递“–forks=#”或“-f #”参数设置，或者在配置文件里面编辑。默认是保守的5个线程。如果你有足够的内存，你可以很容易的设置为50或者更多值。所以经常有人诟病ansible效率太差，是因为他只用到了默认的5个线程。当需要执行大规模的批量操作时，根据需要调高这个参数即可：1ansible -i ./tmp_host all -m shell -a "echo test" -f 30 快固然好，但有时候，我们也需要让我们的程序慢下来。当我们在做灰度发布的时候，往往是一批一批的更新我们的服务。但当我们的服务集群比较小，比如仅有两个实例的互备服务，我们在发布的时候为了不影响用户的体验，我们可以使用“-f”调小并发数，达到平滑发布的效果。1ansible tomcat_server -m shell -a "sh /usr/local/tomcat/bin/shutdown.sh &amp;&amp; sh /usr/local/tomcat/bin/startup.sh" -f 1 inventory的行为参数我们有时候需要在ansible inventory文件中配置ssh的一些参数，我们需要定义主机名，以及ansible的ssh客户端可以连接到的端口(22,2222,22300)等，那么ansible将这些变量命名为inventory的行为参数，如下：123456789名称 默认值 描述ansible_ssh_host 主机的名字 SSH目的主机名或IPansible_ssh_port 22 SSH目的端口ansible_ssh_user root SSH登录使用的用户名ansible_ssh_pass none SSH认证所使用的密码ansible_connection smart ansible使用何种连接模式连接到主机ansible_ssh_private_key_file none SSH认证所使用的私钥ansible_shell_type sh 命令所使用的shellansible_python_interpreter /usr/bin/python 主机上的python解释器 Group Varsgroup vars写在inventory中：1234567[atlanta]host1host2[atlanta:vars]ntp_server=ntp.atlanta.example.comproxy=proxy.atlanta.example.com group_vars文件也可位于一个目录下面，同时在inventory旁边，有一个可选的文件名在每个组后面。这是一个方便的位置来存放变量，提供给每个组，由其是复杂的数据结构，因此这些变量不需要嵌入在inventory文件或playbook文件里面。123File: /etc/ansible/group_vars/groupadmin_user: tomansible_ssh_pass: password Host Varshost vars写在inventory中：123[atlanta]host1 http_port=80 maxRequestsPerChild=808host2 http_port=303 maxRequestsPerChild=909 就像”Group Vars”，也有一个名称为“host_vars/”的目录在inventory文件旁，可以在invetory文件的主机名后面包含这个文件，使用 YAML 格式。这提供一个方便的位置分配变量给这个主机而不要在inventory文件里面嵌入太多变量。Host Vars 文件还可以用于定义复杂的在inventory文件里面不断出现的数据结构。12File: /etc/ansible/host_vars/host1admin_user: john 异步和轮询ansible有时候要执行等待时间很长的操作,这个操作可能要持续很长时间,设置超过ssh的timeout.这时候你可以在step中指定async和poll来实现异步操作async表示这个step的最长等待时长,如果设置为0,表示一直等待下去直到动作完成.poll 表示检查step操作结果的间隔时长.例一：1234567- name: Test hosts: localhost tasks: - name: wair for shell: sleep 16 async: 10 poll: 2 这个step失败, 因为操作时间超过了最大等待时长例二:1234567- name: Test hosts: localhost tasks: - name: wair for shell: sleep 16 async: 10 poll: 0 结果:1234TASK: [wair for] **************************************************************&lt;job 621720484791.102116&gt; finished on localhostPLAY RECAP ******************************************************************** poll 设置为0, 表示不用等待执行结果, 该step执行成功例三:1234567- name: Test hosts: localhost tasks: - name: wair for shell: sleep 16 async: 0 poll: 10 结果:123456789# time ansible-playbook xiama.ymlTASK: [wair for] **************************************************************changed: [localhost]PLAY RECAP ********************************************************************localhost : ok=2 changed=1 unreachable=0 failed=0real 0m16.693s async设置为0, 会一直等待直到该操作完成. 以上。]]></content>
      <categories>
        <category>技术进步</category>
      </categories>
      <tags>
        <tag>tips</tag>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[next主题fullimage显示不居中问题]]></title>
    <url>%2F2018%2F01%2F01%2Ftest-art%2F</url>
    <content type="text"><![CDATA[使用好hexo和next搭建好自己的博客之后，就到了等待备案的日子. 小小抱怨一下不得不说备案真是一个让人抓狂的过程，之前对公司备案的繁琐有所经历，没想到个人备案也是不遑多让。服务器是在阿里云上，阿里云在备案方面帮着省去了很多的备案步骤，结果还是弄得我够呛，关于备案在之前的文章有详细提及。 遇到的问题这次是在阅读iissnan大神文档的时候，发现了FullImage的功能，觉得很酷炫，就想着用着试试。结果一直没达到文档中的效果，图片的尺寸是超出700px了，但是愣是不居中，效果有点尴尬。 解决思路和过程开始猜测是next尺寸的问题，翻了半天，发现自己并没有使用custom.yml调整过尺寸，并且尴尬的是不论怎么调整尺寸都还是一样。秉持着”有啥不懂看官方文档”的宗旨，把作者的文档来来回回看了两遍，看自己的配置是否有问题，结果无功而返。然后直接搜索引擎搜是否有朋友有过类似的困扰，也没有搜到。然后继续寻找NexT主题相关社区，想着提个问题什么的，也没找到靠谱活跃的社区。最终在GitHub上找到了十几天前有位朋友开了个一模一样的issue #2039，并且已经在#2043得到了解决。 以上。]]></content>
      <categories>
        <category>技术进步</category>
      </categories>
      <tags>
        <tag>problems</tag>
        <tag>NexT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F12%2F23%2Fhello-world%2F</url>
    <content type="text"><![CDATA[这是个人网站的第一篇博客，标题就直接用了‘Hello World’。刚开始接触CS的时候，总是略微有些嫌弃‘Hello World’的，可能是这个梗被用得太多，有点嚼烂了的感觉。不过随着时间的推移，越来越喜欢‘Hello world’这个词了。因为每次屏幕上出现的‘Hello World’，总是伴随着新知识的学习，或是新作品的诞生！就像棋手落下的第一子、画家挥下的第一笔…矫情的废话不多说，James’s note主要会记录以下的一些内容。 Tech BlogJames’s note最最主要的作用就是记录关于技术方面的博客，文章，思考，也就是类似于笔记本的作用。关于技术的，不论是长篇大论，还是自己工作中接触到问题的反思和总结,抑或是一个代码片段，都会记录在这边。 Note Life另一方面，主要会记录自己的一些生活经历，出行，有感等等。 Something Else最后，就是一些七七八八的，想要随便写写的杂文了。 At Last希望自己能够多坚持一些，长期，持续地维护好这个主页，在这期间也能持续地不断成长！ More info: HomePage 以上。]]></content>
      <categories>
        <category>杂谈</category>
      </categories>
      <tags>
        <tag>NexT</tag>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[建站请绕道：备案辛酸路]]></title>
    <url>%2F2017%2F10%2F13%2Fbeian%2F</url>
    <content type="text"><![CDATA[自建blog的想法是早就有了的，但是无奈deadline才是第一生产力，这个事情一直拖到前一段时间才得以真正开展。本以为当hexo和NexT两个工具选好了之后，建站对我这个运维来说，不就是分分钟的事情么？可是…我终究还是naive啊。 响应党的号召：备案光荣在namecheap上成功买了域名（PS.不得不说namecheap的服务是真的比狗爹好多了）；就着手在aliyun上买ecs，结果一犯二就买了华南的服务器，当时我并不知道我将会面对什么。修改好dns_server，待其生效便调试了起来，然后就收到党的号召，需要备案。一直以来，我都是一个根正苗红，又红又专的好青年，对于党的这种无理的要求，你问我支不支持，我当然支持。于是我就走上了备案这条不归路。 阿里云建站许久之前刚入行的时候，记得那时候备案是真心麻烦，需要带着你的各种证件，去到专门的“有关部门”，各种拍照，审查，必须经历很长的周期才能成功。现在因为是在阿里云买了服务器，阿里也和审查机构简化了一些流程，现在只需要在阿里云上upload一些文件和证件，似乎就可以备案成功了。 一波三折之所以是“似乎”，是当我开开心心上传各种资料的时候，发现除了身份证、手机号之外，赫然还需要居住证。当时我就蒙蔽了，“身份证信息如非上海市需提供上海市‘临时居住证’或‘居住证’”。难道还专门为了这事去有关部门办一个暂住证？知乎上一问，发现并不是所有地区的备案都需要暂住证，四川、福建等地方是不需要的。所以备案时你所在的地址填这些地方即可。然后顺利拍摄好各种资料上传完成，可不到一天，就被短信告知备案初审就没通过。还是要夸一下阿里云的工作人员的服务，第一时间电话和我沟通，非常仔细地告诉我哪些地方需要修改，然后还给了我一些小建议。还能怎么办呢？挨个解决问题呗。 域名持有者与主办单位不一致其实就是我在namecheap上填写的所有者的名字并不是我的真名（全拼也可以），关于域名的所有者，到任意一个whois网站上可以查询到域名的所有者，当然要放开namecheap上的whois privacy才能查到，你可以试试自己的域名的whois是谁。在namecheap的控制页上，找了半天还找不到修改域名所有者的操作，索性直接找了他们的客服帮忙修改，服务态度也是很好，虽然我的英语是真的烂。 网站名称不合格网站名称不能为“xxx的个人网站”等各种要求，随便改一个名字就可以了。 未取得备案号禁止访问我在申请好域名，配置好DNS之后就尝试着起了web服务调试了起来，好吧，这也是不行的，先关闭呗。 再次提交改好了以上三项，再次提交，很顺利地通过了初审。然后过了大概三天，通知我已经提交管局审核，看起来一切都是那么顺利。结果一天之后，再次传来审核失败的消息。“域名不存在注册商验证库中”，简单来说呢，就是域名是国外的，是有关部门无法掌控的，有关部门希望你把域名转回到国内的服务商来托管。简直了，当时心里有句MMP，也就大声讲了一万多遍。所以摆在我面前的唯一出路，就是将放在namecheap上的域名转回到万网上来。出路是有出路，可是怎么就感觉那么操蛋呢？我只是想简简单单弄个小网站而已…最后我毅然选择不走这条转域名的唯一出路，又弄了一台香港的服务器，项目往上一部署，DNS一修改，齐活。备案？爱谁谁吧！ 经验总结写这么多，经验总结就是老老实实域名服务器都买国外的吧，别把时间浪费在备案上面了。 以上。]]></content>
      <categories>
        <category>杂谈</category>
      </categories>
      <tags>
        <tag>blog</tag>
        <tag>建站</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins+ansible持续集成之自动发布脚本]]></title>
    <url>%2F2017%2F08%2F03%2Fansi-ci%2F</url>
    <content type="text"><![CDATA[工作中使用jenkins已经很久了，很早就想总结一下自己的一些使用情况，本文主要记录一下自动发布的模板脚本。 首先说明jenkins无疑是近几年来最炙手可热的持续集成工具，功能强大，插件应有尽有，社区活跃，且被众多大公司实际生产中应用。关于jenkins，在此就不详细介绍了（主要是我也还远没了解透彻）。要想详细了解jenkins，请直接查看官方文档有详细提及。 主要应用点jenkins是一个功能强大的工具，但我司目前还只是用来做自动拉取代码、打包、发布这几项工作，主要涉及的工具是jenkins和ansible。使用jenkins自动拉取代码，自动构建，ansible完成发布工作。 简单讲解jenkins中主要是使用了maven（针对maven项目）和git两个插件.ansible则可以使用command_line、play_book或者role都可以。由于应用环境简单，我直接使用了ansible的command_line。 jenkins中的shell1sh /deploy/jenkins_deploy_$&#123;JOB_NAME&#125;.sh jenkins-deploy-${JOB_NAME}.sh123456789101112131415161718192021#定义应用变量和应用目录，发布其它应用只需要把APP的值修改成相应的,比如APP=demo.war#需要修改APP这一处即可APP=app_name#定义发布应用的脚本变量，发布其它应用需要把DEPLOY_APP的值修改成相应的，DEPLOY_APP=deploy_$&#123;APP&#125;.sh#定义部署服务器组webservers=$&#123;APP&#125;DEP_PATH=dep_pathPAK_PATH=pak_path#应用服务器创建deploy目录ansible $webservers -u appsvr -m shell -a "mkdir -p $&#123;DEP_PATH&#125;"#将war包复制到应用服务器的/deployansible $webservers -u appsvr -m copy -a "src=$&#123;PAK_PATH&#125;/$&#123;APP&#125;-1.0-SNAPSHOT.jar dest=$&#123;DEP_PATH&#125; owner=appsvr group=appsvr mode=0755 "#将构建ID号复制到应用服务器的/deploy下#ansible $webservers -u qappsom -m copy -a "src=/deploy/$LASTBUILD dest=/deploy/ owner=qappsom group=grpadm mode=0755 "#将部署脚本复制到应用服务器的/deploy下ansible $webservers -u appsvr -m copy -a "src=$&#123;DEP_PATH&#125;/$DEPLOY_APP dest=$&#123;DEP_PATH&#125; owner=appsvr group=appsvr mode=0644 "#执行发布脚本ansible $webservers -u appsvr -m shell -a "sh /$&#123;DEP_PATH&#125;/$&#123;DEPLOY_APP&#125;" -f 1 deploy_${JOB_NAME}.sh1234567891011121314151617181920212223242526272829303132333435363738#如果新增模块，只需复制本脚本，然后把APP的变量值改成相对应的模块名即可。比如,APP=oms#只需要改1处source /etc/profileAPP=app_nameNOW=`date +%Y%m%d-%H%M%S`BACKUP_PATH=back_pathAPP_PATH=app_pathDEP_PATH=dep_pathJAR_NAME=$&#123;APP&#125;-xxx.jar###先确定是否存在备份目录，不存在则创建if [ ! -d "$&#123;BACKUP_PATH&#125;" ]then mkdir -p $&#123;BACKUP_PATH&#125;fi###备份原有jar包，加上时间后缀if [ -f "$&#123;APP_PATH&#125;/$&#123;JAR_NAME&#125;" ]then cp -v $&#123;APP_PATH&#125;/$&#123;JAR_NAME&#125; $&#123;BACKUP_PATH&#125;/$&#123;JAR_NAME&#125;_bak$&#123;NOW&#125; echo "backup is done !"else mkdir -p $&#123;APP_PATH&#125; echo "this is the first time delpoying the $APP !"fi###覆盖应用jar包，重启服务cp -v $&#123;DEP_PATH&#125;$&#123;JAR_NAME&#125; $&#123;APP_PATH&#125;/echo "copying $&#123;JAR_NAME&#125; to $&#123;APP_PATH&#125; down!"until ! [ $(ps -ef|grep "$&#123;JAR_NAME&#125;"|grep -v grep|awk '&#123;print$2&#125;') ];do echo -e "\033[32m [INFO]: killing the $&#123;APP&#125; process ... \033[0m" ps -ef|grep "$&#123;JAR_NAME&#125;"|grep -v grep|awk '&#123;print$2&#125;'|xargs kill sleep 3doneecho "restarting new $&#123;APP&#125; process..."nohup $&#123;JAVA_HOME&#125;/bin/java -jar $&#123;APP_PATH&#125;/$&#123;JAR_NAME&#125; &gt; /dev/null 2&gt;&amp;1 &amp;echo "The $&#123;APP&#125; has stated ! "ps -ef|grep "$&#123;JAR_NAME&#125;"|grep -v grepexit 以上。]]></content>
      <categories>
        <category>技术进步</category>
      </categories>
      <tags>
        <tag>ansible</tag>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GlusterFS简介和可用性测试]]></title>
    <url>%2F2017%2F06%2F03%2Fglusterfs%2F</url>
    <content type="text"><![CDATA[Glusterfs简介GlusterFS是Scale-Out存储解决方案Gluster的核心，它是一个开源的分布式文件系统，具有强大的横向扩展能力，通过扩展能够支持数PB存储容量和处理数千客户端。GlusterFS借助TCP/IP或InfiniBand RDMA（ 远程直接数据存取）网络将物理分布的存储资源聚集在一起，使用单一全局命名空间来管理数据。GlusterFS基于可堆叠的用户空间设计，可为各种不同的数据负载提供优异的性能。 Glusterfs特点扩展性和高性能GlusterFS利用双重特性来提供几TB至数PB的高扩展存储解决方案。Scale-Out架构允许通过简单地增加资源来提高存储容量和性能，磁盘、计算和I/O资源都可以独立增加，支持10GbE和InfiniBand等高速网络互联。Gluster弹性哈希（Elastic Hash）解除了GlusterFS对元数据服务器的需求，消除了单点故障和性能瓶颈，真正实现了并行化数据访问。 高可用性GlusterFS可以对文件进行自动复制，如镜像或多次复制，从而确保数据总是可以访问，甚至是在硬件故障的情况下也能正常访问。自我修复功能能够把数据恢复到正确的状态，而且修复是以增量的方式在后台执行，几乎不会产生性能负载。GlusterFS没有设计自己的私有数据文件格式，而是采用操作系统中主流标准的磁盘文件系统（如EXT，ZFS）来存储文件，因此数据可以使用各种标准工具进行复制和访问。 全局统一命名空间全局统一命名空间将磁盘和内存资源聚集成一个单一的虚拟存储池，对上层用户和应用屏蔽了底层的物理硬件。存储资源可以根据需要在虚拟存储池中进行弹性扩展，比如扩容或收缩。当存储虚拟机映像时，存储的虚拟映像文件没有数量限制，成千虚拟机均通过单一挂载点进行数据共享。虚拟机I/O可在命名空间内的所有服务器上自动进行负载均衡，消除了SAN环境中经常发生的访问热点和性能瓶颈问题。 弹性哈希算法GlusterFS采用弹性哈希算法在存储池中定位数据，而不是采用集中式或分布式元数据服务器索引。在其他的Scale-Out存储系统中，元数据服务器通常会导致I/O性能瓶颈和单点故障问题。GlusterFS中，所有在Scale-Out存储配置中的存储系统都可以智能地定位任意数据分片，不需要查看索引或者向其他服务器查询。这种设计机制完全并行化了数据访问，实现了真正的线性性能扩展。 弹性卷管理数据储存在逻辑卷中，逻辑卷可以从虚拟化的物理存储池进行独立逻辑划分而得到。存储服务器可以在线进行增加和移除，不会导致应用中断。逻辑卷可以在所有配置服务器中增长和缩减，可以在不同服务器迁移进行容量均衡，或者增加和移除系统，这些操作都可在线进行。文件系统配置更改也可以实时在线进行并应用，从而可以适应工作负载条件变化或在线性能调优。 基于标准协议Gluster存储服务支持NFS, CIFS, HTTP, FTP以及Gluster原生协议，完全与POSIX标准兼容。现有应用程序不需要作任何修改或使用专用API，就可以对Gluster中的数据进行访问。这在公有云环境中部署Gluster时非常有用，Gluster对云服务提供商专用API进行抽象，然后提供标准POSIX接口。 glusterfs整体工作流程整体流程如下图所示： 首先是在客户端， 用户通过glusterfs的mount point 来读写数据， 对于用户来说，集群系统的存在对用户是完全透明的，用户感觉不到是操作本地系统还是远端的集群系统。 用户的这个操作被递交给 本地linux系统的VFS来处理。 VFS 将数据递交给FUSE 内核文件系统:在启动 glusterfs 客户端以前，需要想系统注册一个实际的文件系统FUSE,如上图所示，该文件系统与ext3在同一个层次上面， ext3 是对实际的磁盘进行处理， 而fuse 文件系统则是将数据通过/dev/fuse 这个设备文件递交给了glusterfs client端。所以， 我们可以将 fuse文件系统理解为一个代理。 数据被fuse 递交给Glusterfs client 后， client 对数据进行一些指定的处理（所谓的指定，是按照client 配置文件据来进行的一系列处理， 我们在启动glusterfs client 时需要指定这个文件。 在glusterfs client的处理末端，通过网络将数据递交给 Glusterfs Server，并且将数据写入到服务器所控制的存储设备上。 几种volume（卷）的方式distributed volume（分布巻） 分布卷可以将某个文件随机的存储在卷内的一个brick内，通常用于扩展存储能力，不支持数据的冗余。除非底层的brick使用RAID等外部的冗余措施。命令：1$ gluster volume create mamm-volume node1:/media node2:/media node3:/media ... replicated volume（镜像卷） 镜像卷在创建时可指定复本的数量，复本在存储时会在卷的不同brick上，因此有几个复本就必须提供至少多个brick。1$ gluster volume create mamm-volume repl 2 node1:/media node2:/media 注意：在创建复本卷时，brick数量与复本个数必须相等；否则将会报错。另外如果同一个节点提供了多个brick，也可以在同一个结点上创建复本卷，但这并不安全，因为一台设备挂掉，其上面的所有brick就无法访问了。 striped volume（切片卷） 分片卷将单个文件分成小块(块大小支持配置,默认为128K)，然后将小块存储在不同的brick上，以提升文件的访问性能。1$ gluster volume create mamm-volume stripe 2 node1:/media node2:/media stripe后的参数指明切片的分布位置个数注意：brick的个数必须等于分布位置的个数 distribute replication volume（分布式镜像卷） 此类型卷是基本复本卷的扩展，可以指定若干brick组成一个复本卷，另外若干brick组成另个复本卷。单个文件在复本卷内数据保持复制，不同文件在不同复本卷之间进行分布。1$ gluster volume create dr-volume repl 2 node1:/exp1 node2:/exp2 node3:/exp3 node4:/exp4 注意：复本卷的组成依赖于指定brick的顺序brick必须为复本数K的N倍,brick列表将以K个为一组，形成N个复本卷 distribute striped volume 类似于分布式复本卷，若创建的卷的节点提供的bricks个数为stripe个数N倍时，将创建此类型的卷。1$ gluster volume create ds-volume stripe 2 node1:/exp1 node1:/exp2 [&amp;] node2:/exp3 node2:/exp4 注意：切片卷的组成依赖于指定brick的顺序brick必须为复本数K的N倍,brick列表将以K个为一组，形成N个切片卷 striped replicated volume（切片镜像卷） 数据将进行切片，切片在复本卷内进行复制，在不同卷间进行分布。1$ gluster volume create test-volume stripe 2 replica 2 server1:/exp1 server2:/exp2 server3:/exp3 server4:/exp4 exp1和exp2组成复本卷，exp3和exp4组成复本卷，两个复本卷组成分片卷。注意：brick数量必须和stripe个数N和repl参数M的积N*M相等。即对于brick列表，将以M为一组，形成N个切片卷。数据切片分布在N个切片卷上，在每个切片卷内部，切片数据复本M份。 distributed striped replicated vloume（分布式切片镜像卷） 1$ gluster volume create test-volume stripe 2 replica 2 server1:/exp1 server2:/exp2 server3:/exp3 server4:/exp4 server5:/exp5 server6:/exp6 server7:/exp7 server8:/exp8 注意：bricks数量为stripe个数N，和repl个数M的积N*M的整数倍exp1 exp2 exp3 exp4组成一个分布卷，exp1和exp2组成一个stripe卷，exp3和exp4组成另一个stripe卷，1和2，3和4互为复本卷exp4-exp8组成另一个分布卷，略。 读写测试数据及与NFS的比较测试背景： 两者都是使用单客户端请求读写； GlusterFS采用4台2G/2核，20G高性能硬盘的主机，按照分布式切片卷的的方式挂载使用； NFS采用一台2G/2核，20G高性能硬盘的主机，正常RW挂载使用； 从初步测试数据来看，GlusterFS相对于NFS，在读写性能（尤其是写性能）上，有了不错的提升。后期会尝试采用多客户请求，以及其他更多测试工具来进一步测试。 结论：初步看来，GlusterFS在NFS的基础上，在I/O性能，数据可靠性上都有了很好的提升。]]></content>
      <categories>
        <category>技术进步</category>
      </categories>
      <tags>
        <tag>GlusterFS</tag>
        <tag>分布式存储</tag>
      </tags>
  </entry>
</search>
